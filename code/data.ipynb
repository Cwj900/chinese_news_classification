{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382688/382688 [01:25<00:00, 4471.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['京城', '值得', '来场', '文化', '之旅', '博物馆']\n",
      "{101: 'news_culture', 102: 'news_entertainment', 103: 'news_sports', 104: 'news_finance', 106: 'news_house', 107: 'news_car', 108: 'news_edu', 109: 'news_tech', 110: 'news_military', 112: 'news_travel', 113: 'news_world', 115: 'news_agriculture', 116: 'news_game', 114: 'stock', 100: 'news_story'}\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "#获取停用词\n",
    "def get_stopwords(stop_file_name):\n",
    "    with open(stop_file_name, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines=file.readlines()\n",
    "    words=[i.strip() for i in lines]\n",
    "    return words\n",
    "\n",
    "#字符清洗：\n",
    "def text_cleaning(text):\n",
    "    text_result=''\n",
    "    for char in text:\n",
    "        if (char>='\\u4e00' and char<='\\u9fa5') :\n",
    "            text_result+=char\n",
    "    return text_result\n",
    "\n",
    "#数据预处理\n",
    "def co_data(dataset_path,stopwords):\n",
    "    labels=[]\n",
    "    labels_idx=[]\n",
    "    texts=[]\n",
    "\n",
    "    with open(dataset_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines:\n",
    "            parts = line.split(\"_!_\")\n",
    "            labels_idx.append(int(parts[1]))\n",
    "            labels.append(parts[2])\n",
    "            texts.append(parts[3])\n",
    "\n",
    "    #字符清洗\n",
    "    temp=texts.copy()\n",
    "    texts=[]\n",
    "    for text in tqdm(temp):\n",
    "        result=text_cleaning(text)\n",
    "        seg=jieba.cut(result, cut_all=False)\n",
    "        text=[char for char in seg if not char in stopwords]\n",
    "        texts.append(text)\n",
    "    \n",
    "    return texts,labels,labels_idx\n",
    "\n",
    "dataset_path='./dataset/dataset.txt'\n",
    "stopwords=get_stopwords('./dataset/cn_stopwords.txt')\n",
    "#数据预处理\n",
    "texts,labels,labels_idx=co_data(dataset_path,stopwords)\n",
    "print(texts[0])\n",
    "\n",
    "#整理类别和索引\n",
    "def co_labeldict(labels,labels_idx):\n",
    "    \n",
    "    id2labels = {id: label for id, label in zip(labels_idx, labels)}\n",
    "    labels2id = {label: id for id, label in zip(labels_idx, labels)}\n",
    "    return id2labels,labels2id\n",
    "id2labels,labels2id=co_labeldict(labels,labels_idx)\n",
    "print(id2labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载库里的模型自己训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "\n",
    "model = Word2Vec(texts, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.save(r\"model/word2vec.model\")\n",
    "model.wv.save_word2vec_format(r'dataset/word2vec.bin', binary=True)\n",
    "model.wv.save_word2vec_format(r'dataset/word2vec.txt', binary=False)\n",
    "'''\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# 加载保存的模型\n",
    "model = Word2Vec.load(\"model/word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 382688/382688 [00:02<00:00, 168004.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12345, 80, 22741, 227, 1826, 1829, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1]\n",
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "text2indexs=[]\n",
    "max_length=max(len(text) for text in texts)\n",
    "for text in tqdm(texts):\n",
    "    text2index = [model.wv.key_to_index[word] for word in text if word in model.wv.key_to_index]\n",
    "    padded_text2index = text2index + [-1] * (max_length - len(text2index))\n",
    "    text2indexs.append(padded_text2index)\n",
    "print(text2indexs[0])\n",
    "print(len(text2indexs[0]))\n",
    "\n",
    "train_data, rest_data, train_labels, rest_labels = train_test_split(text2indexs, labels_idx, test_size=0.4, random_state=42)\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(rest_data, rest_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# 转换为向量并填充索引 -1 为全零向量\n",
    "def text_to_vector(text, model, embedding_dim=100):\n",
    "    vector = np.zeros((len(text), embedding_dim))\n",
    "    for i, index in enumerate(text):\n",
    "        if index != -1:\n",
    "            vector[i] = model.wv.vectors[index]\n",
    "    return vector\n",
    "\n",
    "train_vec = [text_to_vector(text, model) for text in train_data]\n",
    "val_vec = [text_to_vector(text, model) for text in val_data]\n",
    "test_vec = [text_to_vector(text, model) for text in test_data]\n",
    "\n",
    "\n",
    "# 创建自定义的数据集类\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data)\n",
    "        self.labels=torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        return text, label\n",
    "\n",
    "train_dataset=TextDataset(train_vec,train_labels)\n",
    "val_dataset=TextDataset(val_vec,val_labels)\n",
    "test_dataset=TextDataset(test_vec,test_labels)\n",
    "\n",
    "train_dataloader=DataLoader(train_dataset,32,shuffle=True)\n",
    "val_dataloader=DataLoader(val_dataset,32,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载训练好的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载词嵌入模型\n",
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('./sogou/sgns.sogou.word', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "text2indexs=[]\n",
    "max_length=max(len(text) for text in texts)\n",
    "for text in tqdm(texts):\n",
    "    text2index = [model.key_to_index[word] for word in text if word in model.key_to_index]\n",
    "    padded_text2index = text2index + [-1] * (max_length - len(text2index))\n",
    "    text2indexs.append(padded_text2index)\n",
    "print(text2indexs[0])\n",
    "print(len(text2indexs[0]))\n",
    "\n",
    "train_data, rest_data, train_labels, rest_labels = train_test_split(text2indexs, labels_idx, test_size=0.4, random_state=42)\n",
    "val_data, test_data, val_labels, test_labels = train_test_split(rest_data, rest_labels, test_size=0.5, random_state=42)\n",
    "\n",
    "# 转换为向量并填充索引 -1 为全零向量\n",
    "def text_to_vector(text, model, embedding_dim=300):\n",
    "    vector = np.zeros((len(text), embedding_dim))\n",
    "    for i, index in enumerate(text):\n",
    "        if index != -1:\n",
    "            vector[i] = model[index]\n",
    "    return vector\n",
    "\n",
    "train_vec = [text_to_vector(text, model) for text in train_data]\n",
    "val_vec = [text_to_vector(text, model) for text in val_data]\n",
    "test_vec = [text_to_vector(text, model) for text in test_data]\n",
    "\n",
    "\n",
    "# 创建自定义的数据集类\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data)\n",
    "        self.labels=torch.tensor(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        return text, label\n",
    "\n",
    "train_dataset=TextDataset(train_vec,train_labels)\n",
    "val_dataset=TextDataset(val_vec,val_labels)\n",
    "test_dataset=TextDataset(test_vec,test_labels)\n",
    "\n",
    "train_dataloader=DataLoader(train_dataset,32,shuffle=True)\n",
    "val_dataloader=DataLoader(val_dataset,32,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, labels in train_dataloader:\n",
    "    \n",
    "    print(inputs.shape)\n",
    "    print(type(inputs))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 因为转换后的向量inputs是浮点型（没转换为int型，担心这样会导致数据有效性降低，因为转换后一些不同的值会相同），所以CNN中无法使用嵌入层，能否改为全连接层或其他层或去掉\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self,num_classes,embedding_dim=100) :\n",
    "        super(CNN_LSTM,self).__init__()\n",
    "        #嵌入\n",
    "        self.embedding = nn.Embedding(7000+2, embedding_dim)\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, 64, kernel_size=3, padding=1)\n",
    "        self.maxpool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.maxpool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.lstm = nn.LSTM(64, 100, dropout=0.2, batch_first=True)\n",
    "        self.fc = nn.Linear(100, num_classes)\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedded = self.embedding(inputs)\n",
    "        embedded = embedded.permute(0, 2, 1)\n",
    "        out = self.conv1(embedded)\n",
    "        out = torch.relu(out)\n",
    "        out = self.maxpool1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.maxpool2(out)\n",
    "        out, _ = self.lstm(out.transpose(1, 2))\n",
    "        out = out[:, -1, :]  # 取最后一个时间步\n",
    "        output = self.fc(out)\n",
    "        return output\n",
    "    \n",
    "model = CNN_LSTM (15) # 创建模型实例\n",
    "loss_fn = nn.CrossEntropyLoss()  # 定义损失函数\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # 定义优化器\n",
    "\n",
    "# 步骤3：训练模型\n",
    "max_epochs = 10\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    model.train()  # 设置模型为训练模式\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs = torch.flatten(inputs, start_dim=1)\n",
    "\n",
    "        optimizer.zero_grad()  # 梯度清零\n",
    "\n",
    "        # 前向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 步骤4：评估模型\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "\n",
    "        for inputs, labels in val_dataloader:\n",
    "            inputs = torch.flatten(inputs, start_dim=1)\n",
    "            # 前向传播\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "        accuracy = total_correct / total_samples\n",
    "        print(f\"Epoch {epoch+1}/{max_epochs}, Validation Accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
